{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"info_train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>prompt</th>\n",
       "      <th>mos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DALLE2_0000.png</td>\n",
       "      <td>A cyberpunk gopnik on the street of a Soviet s...</td>\n",
       "      <td>3.663887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DALLE2_0002.png</td>\n",
       "      <td>portrait of a girl in her 20 s with wavy black...</td>\n",
       "      <td>4.173802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DALLE2_0003.png</td>\n",
       "      <td>a futuristic cowboy firing a glowing revolver ...</td>\n",
       "      <td>2.999392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DALLE2_0004.png</td>\n",
       "      <td>robotic pit bull with a rocket launcher, reali...</td>\n",
       "      <td>3.644606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DALLE2_0005.png</td>\n",
       "      <td>luminous magical paper scroll floating in the ...</td>\n",
       "      <td>3.984836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name                                             prompt  \\\n",
       "0  DALLE2_0000.png  A cyberpunk gopnik on the street of a Soviet s...   \n",
       "1  DALLE2_0002.png  portrait of a girl in her 20 s with wavy black...   \n",
       "2  DALLE2_0003.png  a futuristic cowboy firing a glowing revolver ...   \n",
       "3  DALLE2_0004.png  robotic pit bull with a rocket launcher, reali...   \n",
       "4  DALLE2_0005.png  luminous magical paper scroll floating in the ...   \n",
       "\n",
       "        mos  \n",
       "0  3.663887  \n",
       "1  4.173802  \n",
       "2  2.999392  \n",
       "3  3.644606  \n",
       "4  3.984836  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.columns = ['name', 'prompt', 'mos']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    image_path = \"train\"\n",
    "    captions_path = \".\"\n",
    "    batch_size = 16\n",
    "    num_workers = 4\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 20\n",
    "    device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, prompts,mos, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.mos = mos\n",
    "        self.captions = list(prompts)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(prompts), padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "        item['mos'] = torch.tensor(self.mos[idx]/5).float()\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "def get_transforms(mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.classification_model = torch.nn.Sequential( \n",
    "                torch.nn.Linear(in_features = 256, out_features = 1), \n",
    "                torch.nn.Sigmoid() \n",
    "            )\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        mos_scores = batch[\"mos\"]\n",
    "        \n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        output_linear = self.classification_model(image_embeddings) \n",
    "        # print(output_linear)      \n",
    "        # Calculating the Loss\n",
    "        embeddings_similarity = (self.cos(image_embeddings, text_embeddings)+1)/2\n",
    "        sim_loss = self.mse_loss(embeddings_similarity, mos_scores)\n",
    "        cls_loss = self.mse_loss(output_linear, mos_scores)\n",
    "        \n",
    "        loss = sim_loss + cls_loss\n",
    "        # logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        # images_similarity = image_embeddings @ image_embeddings.T\n",
    "        # texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        # targets = F.softmax(\n",
    "        #     (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        # )\n",
    "        # texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        # images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        # loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = get_transforms(mode=mode)\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"name\"].values,\n",
    "        dataframe[\"prompt\"].values,\n",
    "        dataframe[\"mos\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =train_data\n",
    "valid_df = valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # train_df, valid_df = make_train_valid_dfs()\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "\n",
    "    model = CLIPModel().to(CFG.device)\n",
    "    params = [\n",
    "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.image_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(model, valid_loader)\n",
    "        \n",
    "        if valid_loss.avg < best_loss:\n",
    "            best_loss = valid_loss.avg\n",
    "            torch.save(model.state_dict(), \"best.pt\")\n",
    "            print(\"Saved Best Model!\")\n",
    "        \n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744a02b141e142469186099915377785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3131],\n",
      "        [0.4824],\n",
      "        [0.5559],\n",
      "        [0.3889],\n",
      "        [0.3492],\n",
      "        [0.3789],\n",
      "        [0.3072],\n",
      "        [0.3135],\n",
      "        [0.2825],\n",
      "        [0.6156],\n",
      "        [0.3328],\n",
      "        [0.3709],\n",
      "        [0.6450],\n",
      "        [0.2937],\n",
      "        [0.3771],\n",
      "        [0.3934]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hossein/anaconda3/envs/ann/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9988],\n",
      "        [0.9983],\n",
      "        [0.9986],\n",
      "        [0.9984],\n",
      "        [0.9987],\n",
      "        [0.9971],\n",
      "        [0.9983],\n",
      "        [0.9981],\n",
      "        [0.9987],\n",
      "        [0.9987],\n",
      "        [0.9984],\n",
      "        [0.9990],\n",
      "        [0.9981],\n",
      "        [0.9981],\n",
      "        [0.9985],\n",
      "        [0.9975]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9989],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9990],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9989],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9990],\n",
      "        [0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9991],\n",
      "        [0.9992],\n",
      "        [0.9994],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9991],\n",
      "        [0.9991],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9992]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9992],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9992],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9993],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9992]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9992],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9993],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9991],\n",
      "        [0.9990],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9990],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9992]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9992],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9990],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9991],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9992],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9991],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9989],\n",
      "        [0.9991],\n",
      "        [0.9989],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9989],\n",
      "        [0.9991],\n",
      "        [0.9989],\n",
      "        [0.9988],\n",
      "        [0.9988],\n",
      "        [0.9990],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9991],\n",
      "        [0.9991],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9991],\n",
      "        [0.9989],\n",
      "        [0.9988],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9989],\n",
      "        [0.9988],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9989],\n",
      "        [0.9988],\n",
      "        [0.9988],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9988],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9991],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9989],\n",
      "        [0.9988],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9990],\n",
      "        [0.9988],\n",
      "        [0.9989],\n",
      "        [0.9988],\n",
      "        [0.9990],\n",
      "        [0.9990],\n",
      "        [0.9988],\n",
      "        [0.9986],\n",
      "        [0.9988],\n",
      "        [0.9991],\n",
      "        [0.9990],\n",
      "        [0.9989]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9988],\n",
      "        [0.9988],\n",
      "        [0.9990],\n",
      "        [0.9987],\n",
      "        [0.9986],\n",
      "        [0.9989],\n",
      "        [0.9988],\n",
      "        [0.9990],\n",
      "        [0.9988],\n",
      "        [0.9990],\n",
      "        [0.9987],\n",
      "        [0.9988],\n",
      "        [0.9988],\n",
      "        [0.9987],\n",
      "        [0.9989],\n",
      "        [0.9987]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9989],\n",
      "        [0.9988],\n",
      "        [0.9987],\n",
      "        [0.9990],\n",
      "        [0.9986],\n",
      "        [0.9988],\n",
      "        [0.9987],\n",
      "        [0.9989],\n",
      "        [0.9985],\n",
      "        [0.9987],\n",
      "        [0.9986],\n",
      "        [0.9990],\n",
      "        [0.9987],\n",
      "        [0.9986],\n",
      "        [0.9987],\n",
      "        [0.9989]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9987],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9987],\n",
      "        [0.9985],\n",
      "        [0.9986],\n",
      "        [0.9989],\n",
      "        [0.9987],\n",
      "        [0.9988],\n",
      "        [0.9988],\n",
      "        [0.9987],\n",
      "        [0.9989],\n",
      "        [0.9989],\n",
      "        [0.9985],\n",
      "        [0.9987],\n",
      "        [0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9988],\n",
      "        [0.9986],\n",
      "        [0.9985],\n",
      "        [0.9984],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9988],\n",
      "        [0.9988],\n",
      "        [0.9984],\n",
      "        [0.9987],\n",
      "        [0.9986],\n",
      "        [0.9988]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9986],\n",
      "        [0.9985],\n",
      "        [0.9984],\n",
      "        [0.9984],\n",
      "        [0.9985],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9985],\n",
      "        [0.9987],\n",
      "        [0.9989],\n",
      "        [0.9985],\n",
      "        [0.9988],\n",
      "        [0.9988],\n",
      "        [0.9987],\n",
      "        [0.9987]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9984],\n",
      "        [0.9984],\n",
      "        [0.9986],\n",
      "        [0.9986],\n",
      "        [0.9985],\n",
      "        [0.9983],\n",
      "        [0.9983],\n",
      "        [0.9984],\n",
      "        [0.9983],\n",
      "        [0.9984],\n",
      "        [0.9986],\n",
      "        [0.9988],\n",
      "        [0.9986],\n",
      "        [0.9983],\n",
      "        [0.9986],\n",
      "        [0.9984]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9986],\n",
      "        [0.9987],\n",
      "        [0.9985],\n",
      "        [0.9986],\n",
      "        [0.9985],\n",
      "        [0.9984],\n",
      "        [0.9981],\n",
      "        [0.9986],\n",
      "        [0.9985],\n",
      "        [0.9981],\n",
      "        [0.9986],\n",
      "        [0.9987],\n",
      "        [0.9982],\n",
      "        [0.9983],\n",
      "        [0.9984],\n",
      "        [0.9984]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9983],\n",
      "        [0.9984],\n",
      "        [0.9981],\n",
      "        [0.9983],\n",
      "        [0.9985],\n",
      "        [0.9984],\n",
      "        [0.9985],\n",
      "        [0.9983],\n",
      "        [0.9982],\n",
      "        [0.9983],\n",
      "        [0.9984],\n",
      "        [0.9983],\n",
      "        [0.9984],\n",
      "        [0.9979],\n",
      "        [0.9982],\n",
      "        [0.9984]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9983],\n",
      "        [0.9978],\n",
      "        [0.9980],\n",
      "        [0.9982],\n",
      "        [0.9982],\n",
      "        [0.9978],\n",
      "        [0.9982],\n",
      "        [0.9983],\n",
      "        [0.9983],\n",
      "        [0.9977],\n",
      "        [0.9979],\n",
      "        [0.9983],\n",
      "        [0.9984],\n",
      "        [0.9981],\n",
      "        [0.9983],\n",
      "        [0.9983]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9980],\n",
      "        [0.9979],\n",
      "        [0.9981],\n",
      "        [0.9978],\n",
      "        [0.9979],\n",
      "        [0.9983],\n",
      "        [0.9982],\n",
      "        [0.9982],\n",
      "        [0.9980],\n",
      "        [0.9980],\n",
      "        [0.9982],\n",
      "        [0.9979],\n",
      "        [0.9981],\n",
      "        [0.9983],\n",
      "        [0.9979],\n",
      "        [0.9983]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9979],\n",
      "        [0.9980],\n",
      "        [0.9981],\n",
      "        [0.9982],\n",
      "        [0.9976],\n",
      "        [0.9980],\n",
      "        [0.9981],\n",
      "        [0.9980],\n",
      "        [0.9978],\n",
      "        [0.9981],\n",
      "        [0.9979],\n",
      "        [0.9978],\n",
      "        [0.9977],\n",
      "        [0.9980],\n",
      "        [0.9978],\n",
      "        [0.9977]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9976],\n",
      "        [0.9983],\n",
      "        [0.9979],\n",
      "        [0.9978],\n",
      "        [0.9975],\n",
      "        [0.9976],\n",
      "        [0.9978],\n",
      "        [0.9981],\n",
      "        [0.9978],\n",
      "        [0.9979],\n",
      "        [0.9976],\n",
      "        [0.9977],\n",
      "        [0.9978],\n",
      "        [0.9978],\n",
      "        [0.9981],\n",
      "        [0.9980]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9978],\n",
      "        [0.9978],\n",
      "        [0.9978],\n",
      "        [0.9976],\n",
      "        [0.9972],\n",
      "        [0.9978],\n",
      "        [0.9978],\n",
      "        [0.9976],\n",
      "        [0.9978],\n",
      "        [0.9976],\n",
      "        [0.9976],\n",
      "        [0.9977],\n",
      "        [0.9968],\n",
      "        [0.9981],\n",
      "        [0.9975],\n",
      "        [0.9972]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9974],\n",
      "        [0.9973],\n",
      "        [0.9979],\n",
      "        [0.9980],\n",
      "        [0.9969],\n",
      "        [0.9973],\n",
      "        [0.9979],\n",
      "        [0.9971],\n",
      "        [0.9971],\n",
      "        [0.9975],\n",
      "        [0.9974],\n",
      "        [0.9970],\n",
      "        [0.9978],\n",
      "        [0.9975],\n",
      "        [0.9973],\n",
      "        [0.9968]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9974],\n",
      "        [0.9972],\n",
      "        [0.9975],\n",
      "        [0.9973],\n",
      "        [0.9976],\n",
      "        [0.9971],\n",
      "        [0.9976],\n",
      "        [0.9975],\n",
      "        [0.9971],\n",
      "        [0.9968],\n",
      "        [0.9974],\n",
      "        [0.9969],\n",
      "        [0.9972],\n",
      "        [0.9967],\n",
      "        [0.9975],\n",
      "        [0.9967]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9979],\n",
      "        [0.9970],\n",
      "        [0.9973],\n",
      "        [0.9972],\n",
      "        [0.9977],\n",
      "        [0.9966],\n",
      "        [0.9972],\n",
      "        [0.9972],\n",
      "        [0.9972],\n",
      "        [0.9967],\n",
      "        [0.9973],\n",
      "        [0.9968],\n",
      "        [0.9964],\n",
      "        [0.9963],\n",
      "        [0.9968],\n",
      "        [0.9968]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9975],\n",
      "        [0.9968],\n",
      "        [0.9965],\n",
      "        [0.9971],\n",
      "        [0.9969],\n",
      "        [0.9957],\n",
      "        [0.9969],\n",
      "        [0.9961],\n",
      "        [0.9975],\n",
      "        [0.9967],\n",
      "        [0.9966],\n",
      "        [0.9968],\n",
      "        [0.9975],\n",
      "        [0.9962],\n",
      "        [0.9967],\n",
      "        [0.9961]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9966],\n",
      "        [0.9964],\n",
      "        [0.9962],\n",
      "        [0.9965],\n",
      "        [0.9968],\n",
      "        [0.9967],\n",
      "        [0.9970],\n",
      "        [0.9960],\n",
      "        [0.9959],\n",
      "        [0.9963],\n",
      "        [0.9961],\n",
      "        [0.9971],\n",
      "        [0.9959],\n",
      "        [0.9960],\n",
      "        [0.9970],\n",
      "        [0.9967]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9968],\n",
      "        [0.9960],\n",
      "        [0.9960],\n",
      "        [0.9964],\n",
      "        [0.9959],\n",
      "        [0.9948],\n",
      "        [0.9961],\n",
      "        [0.9956],\n",
      "        [0.9953],\n",
      "        [0.9949],\n",
      "        [0.9964],\n",
      "        [0.9957],\n",
      "        [0.9957],\n",
      "        [0.9955],\n",
      "        [0.9968],\n",
      "        [0.9966]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9960],\n",
      "        [0.9954],\n",
      "        [0.9949],\n",
      "        [0.9950],\n",
      "        [0.9961],\n",
      "        [0.9948],\n",
      "        [0.9962],\n",
      "        [0.9951],\n",
      "        [0.9947],\n",
      "        [0.9953],\n",
      "        [0.9964],\n",
      "        [0.9954],\n",
      "        [0.9957],\n",
      "        [0.9961],\n",
      "        [0.9956],\n",
      "        [0.9953]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9939],\n",
      "        [0.9933],\n",
      "        [0.9945],\n",
      "        [0.9956],\n",
      "        [0.9938],\n",
      "        [0.9934],\n",
      "        [0.9953],\n",
      "        [0.9963],\n",
      "        [0.9946],\n",
      "        [0.9945],\n",
      "        [0.9962],\n",
      "        [0.9958],\n",
      "        [0.9938],\n",
      "        [0.9955],\n",
      "        [0.9954],\n",
      "        [0.9959]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9942],\n",
      "        [0.9956],\n",
      "        [0.9933],\n",
      "        [0.9941],\n",
      "        [0.9946],\n",
      "        [0.9939],\n",
      "        [0.9943],\n",
      "        [0.9920],\n",
      "        [0.9944],\n",
      "        [0.9939],\n",
      "        [0.9953],\n",
      "        [0.9951],\n",
      "        [0.9937],\n",
      "        [0.9927],\n",
      "        [0.9945],\n",
      "        [0.9928]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9947],\n",
      "        [0.9934],\n",
      "        [0.9945],\n",
      "        [0.9917],\n",
      "        [0.9911],\n",
      "        [0.9912],\n",
      "        [0.9932],\n",
      "        [0.9953],\n",
      "        [0.9930],\n",
      "        [0.9949],\n",
      "        [0.9950],\n",
      "        [0.9937],\n",
      "        [0.9914],\n",
      "        [0.9948],\n",
      "        [0.9936],\n",
      "        [0.9894]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9923],\n",
      "        [0.9902],\n",
      "        [0.9922],\n",
      "        [0.9926],\n",
      "        [0.9938],\n",
      "        [0.9924],\n",
      "        [0.9937],\n",
      "        [0.9903],\n",
      "        [0.9926],\n",
      "        [0.9905],\n",
      "        [0.9903],\n",
      "        [0.9920],\n",
      "        [0.9904],\n",
      "        [0.9905],\n",
      "        [0.9936],\n",
      "        [0.9916]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9911],\n",
      "        [0.9890],\n",
      "        [0.9878],\n",
      "        [0.9897],\n",
      "        [0.9897],\n",
      "        [0.9884],\n",
      "        [0.9915],\n",
      "        [0.9915],\n",
      "        [0.9894],\n",
      "        [0.9915],\n",
      "        [0.9905],\n",
      "        [0.9920],\n",
      "        [0.9914],\n",
      "        [0.9916],\n",
      "        [0.9924],\n",
      "        [0.9893]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9907],\n",
      "        [0.9908],\n",
      "        [0.9863],\n",
      "        [0.9913],\n",
      "        [0.9883],\n",
      "        [0.9893],\n",
      "        [0.9885],\n",
      "        [0.9879],\n",
      "        [0.9908],\n",
      "        [0.9864],\n",
      "        [0.9872],\n",
      "        [0.9892],\n",
      "        [0.9914],\n",
      "        [0.9891],\n",
      "        [0.9862],\n",
      "        [0.9898]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9868],\n",
      "        [0.9831],\n",
      "        [0.9862],\n",
      "        [0.9813],\n",
      "        [0.9879],\n",
      "        [0.9804],\n",
      "        [0.9868],\n",
      "        [0.9843],\n",
      "        [0.9836],\n",
      "        [0.9863],\n",
      "        [0.9897],\n",
      "        [0.9785],\n",
      "        [0.9849],\n",
      "        [0.9814],\n",
      "        [0.9866],\n",
      "        [0.9849]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9834],\n",
      "        [0.9775],\n",
      "        [0.9774],\n",
      "        [0.9802],\n",
      "        [0.9803],\n",
      "        [0.9746],\n",
      "        [0.9833],\n",
      "        [0.9831],\n",
      "        [0.9811],\n",
      "        [0.9821],\n",
      "        [0.9769],\n",
      "        [0.9837],\n",
      "        [0.9858],\n",
      "        [0.9765],\n",
      "        [0.9868],\n",
      "        [0.9775]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9698],\n",
      "        [0.9845],\n",
      "        [0.9731],\n",
      "        [0.9738],\n",
      "        [0.9712],\n",
      "        [0.9762],\n",
      "        [0.9706],\n",
      "        [0.9726],\n",
      "        [0.9721],\n",
      "        [0.9845],\n",
      "        [0.9710],\n",
      "        [0.9681],\n",
      "        [0.9662],\n",
      "        [0.9721],\n",
      "        [0.9696],\n",
      "        [0.9732]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9597],\n",
      "        [0.9621],\n",
      "        [0.9618],\n",
      "        [0.9563],\n",
      "        [0.9616],\n",
      "        [0.9540],\n",
      "        [0.9702],\n",
      "        [0.9649],\n",
      "        [0.9663],\n",
      "        [0.9560],\n",
      "        [0.9678],\n",
      "        [0.9612],\n",
      "        [0.9685],\n",
      "        [0.9692],\n",
      "        [0.9553],\n",
      "        [0.9604]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9349],\n",
      "        [0.9403],\n",
      "        [0.9446],\n",
      "        [0.9384],\n",
      "        [0.9396],\n",
      "        [0.9540],\n",
      "        [0.9275],\n",
      "        [0.9358],\n",
      "        [0.9353],\n",
      "        [0.9432],\n",
      "        [0.9602],\n",
      "        [0.9261],\n",
      "        [0.9519],\n",
      "        [0.9669],\n",
      "        [0.9517],\n",
      "        [0.9528]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9067],\n",
      "        [0.8822],\n",
      "        [0.8675],\n",
      "        [0.9026],\n",
      "        [0.9046],\n",
      "        [0.8886],\n",
      "        [0.8889],\n",
      "        [0.8942],\n",
      "        [0.8827],\n",
      "        [0.8926],\n",
      "        [0.8699],\n",
      "        [0.9246],\n",
      "        [0.8598],\n",
      "        [0.9020],\n",
      "        [0.9174],\n",
      "        [0.9419]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7850],\n",
      "        [0.8136],\n",
      "        [0.8154],\n",
      "        [0.7971],\n",
      "        [0.7475],\n",
      "        [0.8174],\n",
      "        [0.7871],\n",
      "        [0.8789],\n",
      "        [0.8495],\n",
      "        [0.8011],\n",
      "        [0.8648],\n",
      "        [0.8092],\n",
      "        [0.8543],\n",
      "        [0.8741],\n",
      "        [0.8102],\n",
      "        [0.7628]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7903],\n",
      "        [0.7232],\n",
      "        [0.6559],\n",
      "        [0.7570],\n",
      "        [0.7208],\n",
      "        [0.7186],\n",
      "        [0.7837],\n",
      "        [0.7651],\n",
      "        [0.7089],\n",
      "        [0.6776],\n",
      "        [0.6545],\n",
      "        [0.7227],\n",
      "        [0.6969],\n",
      "        [0.8127],\n",
      "        [0.8138],\n",
      "        [0.7893]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5601],\n",
      "        [0.6250],\n",
      "        [0.6639],\n",
      "        [0.6322],\n",
      "        [0.6288],\n",
      "        [0.6668],\n",
      "        [0.7395],\n",
      "        [0.6364],\n",
      "        [0.6330],\n",
      "        [0.6313],\n",
      "        [0.5974],\n",
      "        [0.6357],\n",
      "        [0.7455],\n",
      "        [0.6420],\n",
      "        [0.7082],\n",
      "        [0.6366]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6787],\n",
      "        [0.6557],\n",
      "        [0.6287],\n",
      "        [0.6925],\n",
      "        [0.6936],\n",
      "        [0.7227],\n",
      "        [0.7526],\n",
      "        [0.6335],\n",
      "        [0.6689],\n",
      "        [0.7193],\n",
      "        [0.6487],\n",
      "        [0.6466],\n",
      "        [0.7107],\n",
      "        [0.7013],\n",
      "        [0.6157],\n",
      "        [0.6997]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[16], line 26\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 26\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, step)\u001b[0m\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m model(batch)\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ann/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ann/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
